{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear Credibility\n",
    "\n",
    "The key to interpretable machine learning\n",
    "\n",
    "1. explanation should be interpretable\n",
    "2. number of explanations should be low (low entropy on possible explanations)\n",
    "\n",
    "Assume $D = \\{(x_i, y_i)\\}_{i=1}^{|D|}$, then we have the following optimization function (denote $z$ as explanation)\n",
    "\n",
    "$cost(D) = \\mathbb{E}_{x,y,z} L(x, y, z) + \\alpha \\mathcal{H}(z)$\n",
    "\n",
    "where $x$ is the input, $y$ is the output, $L$ is the data loss function, and $\\mathcal{H}(z)$ is entropy and $\\alpha>0$ is tradeoff parameter\n",
    "\n",
    "Based on this design guideline, we propose the following generalized linear interpretable model:\n",
    "\n",
    "![model_pipeline](presentations/unifiying_view_credibility.png )\n",
    "\n",
    "The model has two components, the switch network (parametrized by $\\theta_s$ and output explanation probability $p(z|x)$) and the weight network (parametrized by $\\theta_w$ and output parameters of an interpretable model f). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the switch network and the function network are differentiable, we can derive the update gradient for $\\theta_s$ and $\\theta_w$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial{cost(D)}}{\\partial \\theta_s} &= \\frac{\\partial \\mathbb{E}_{x,y,z} L(x, y, z) + \\alpha \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\frac{\\partial \\sum_{x,y,z} p(x,y,z) L(x, y, z) + \\alpha \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\frac{\\partial \\sum_{x,y,z} p(x) p(y|x) p(z|x,y) L(x, y, z) + \\alpha \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\frac{\\partial \\sum_{x,y,z} p(x,y) p(z|x) L(x, y, z) + \\alpha \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\sum_{z} L(x, y, z) \\frac{ \\partial p(z|x)}{\\partial \\theta_s} + \\alpha  \\frac{\\partial \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} + \\alpha \\frac{\\partial \\mathcal{H}(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\frac{\\partial \\sum_{z} p(z) \\log p(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{z} (\\log p(z) \\frac{\\partial p(z)}{\\partial \\theta_s} + p(z) \\frac{\\partial \\log p(z)}{\\partial \\theta_s}) \\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{z} (\\log p(z) + 1) \\frac{\\partial p(z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{z} (\\log p(z) + 1) \\frac{\\partial \\sum_{x,y} p(x,y,z)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{z} (\\log p(z) + 1) \\frac{\\partial \\sum_{x,y} p(x,y)p(z|x)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{z} (\\log p(z) + 1) \\sum_{x,y} p(x,y) \\frac{\\partial p(z|x)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\sum_{x,y} p(x,y) \\sum_{z} (\\log p(z) + 1) \\frac{\\partial p(z|x)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\mathbb{E}_{x,y} \\sum_{z} (\\log p(z) + 1) \\frac{\\partial p(z|x)}{\\partial \\theta_s} \\frac{p(z|x)}{p(z|x)}\\\\\n",
    "    &= \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} L(x, y, z) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s} - \\alpha \\mathbb{E}_{x,y} \\mathbb{E}_{z|x} (\\log p(z) + 1) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s}\\\\\n",
    "    &= \\mathbb{E}_{x,y}\\mathbb{E}_{z|x} (L(x, y, z) - \\alpha \\log p(z) - \\alpha) \\frac{\\partial \\log p(z|x)}{\\partial \\theta_s}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial{cost(D)}}{\\partial \\theta_w} &= \\frac{\\partial \\mathbb{E}_{x,y,z} L(x, y, z) + \\alpha \\mathcal{H}(z)}{\\partial \\theta_w}\\\\\n",
    "&= \\frac{\\partial \\mathbb{E}_{x,y,z} L(x, y, z)}{\\partial \\theta_w}\\\\\n",
    "&= \\mathbb{E}_{x,y,z} \\frac{\\partial L(x, y, z)}{\\partial \\theta_w}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## design choices\n",
    "\n",
    "We consider 3 variant  of the switch network (assuming $z \\in \\mathbb{R}^K$)\n",
    "\n",
    "1. the last layer of the switch network is a softmax ($z$ being a one hot vector)\n",
    "2. the last layer of the switch network is multiple sigmoid functions ($p(z|x) = \\Pi_{k=1}^K p(z_k|x)$)\n",
    "3. the switch network is implemented by an rnn ($p(z|x) = \\Pi_{k=1}^K p(z_k|x,z_{1:k-1})$)\n",
    "\n",
    "Obviously, variant 3 is the most general because it computes the full probability while variant 2 assumes digits of z  are independently generated given the input. Variant 1 is also general but it can only hold $K$ explanations, whereas the other two can hold $2^K$ number of explanations so that they are more memory efficient.\n",
    "\n",
    "We consider the following environment to run our experiment\n",
    "\n",
    "- Triangle world (to test a world where only 3 line suffices)\n",
    "- Egg world (to test a world where potentially infinite lines are needed)\n",
    "- Island in the sea (to fail input gradient)\n",
    "- Big and small islands (to fail counterfactual explanation)\n",
    "- Randomly many islands (to get intuition on the model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
